{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import tifffile\n",
    "from skimage import io\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from glob import glob\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH='input/breast-cancer-cell-segmentation/'\n",
    "# Concat path\n",
    "IMAGES_PATH = osp.join(BASE_PATH, 'Images')\n",
    "LABELS_PATH = osp.join(BASE_PATH, 'Masks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all tif files\n",
    "imgs_paths = glob(osp.join(IMAGES_PATH,\"*.tif\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all mask label\n",
    "masks_paths = [osp.join(LABELS_PATH, i.rsplit(\"/\",1)[-1].split(\"_ccd\")[0]+\".TIF\") for i in imgs_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Couple img/mask\n",
    "img_mask_tuples = list(zip(imgs_paths, masks_paths))\n",
    "\n",
    "# Random\n",
    "random.shuffle(img_mask_tuples)\n",
    "\n",
    "# Split training set 75/25\n",
    "train_tuples, test_tuples = train_test_split(img_mask_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tiff_image(path, normalized=True, resize=(512, 512)):\n",
    "    \"\"\"\n",
    "    Read, preprocess, and optionally normalize a TIFF image.\n",
    "\n",
    "    Parameters:\n",
    "    - path (str): The file path to the TIFF image that needs to be loaded and processed.\n",
    "    - normalized (bool, optional): A flag indicating whether the image should be normalized or not. Default is True.\n",
    "    - resize (tuple, optional): A tuple specifying the target dimensions (width, height) for resizing the image. Default is (512, 512).\n",
    "\n",
    "    Returns:\n",
    "    - If normalized=True, the function returns the preprocessed image with pixel values scaled between 0 and 1 (normalized).\n",
    "    - If normalized=False, the function returns the preprocessed image without normalization, retaining its original pixel values.\n",
    "    \"\"\"\n",
    "    # Step 1: Read the TIFF image from the specified path\n",
    "    image = io.imread(path)\n",
    "    \n",
    "    # Step 2: Convert the image from BGR to RGB format\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Step 3: Resize the image to the specified dimensions\n",
    "    image = cv2.resize(image, resize)\n",
    "    \n",
    "    # Step 4: Normalize the image if 'normalized' is True\n",
    "    if normalized:\n",
    "        return image / 255.0\n",
    "    \n",
    "    # Step 5: If 'normalized' is False, return the original image\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch dataset class for binary classification of images and masks.\n",
    "\n",
    "    Parameters:\n",
    "    - img_mask_tuples (list or tuple): A list of tuples, where each tuple contains the file paths to an image and its corresponding mask.\n",
    "\n",
    "    Methods:\n",
    "    - __init__(self, img_mask_tuples): The class constructor, initializes the dataset with the provided image-mask tuples.\n",
    "    - __len__(self): Returns the total number of image-mask tuples in the dataset.\n",
    "    - __getitem__(self, idx): Fetches and preprocesses the image-mask pair at the given index.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_mask_tuples):\n",
    "        \"\"\"\n",
    "        Constructor method to initialize the dataset with image-mask tuples.\n",
    "\n",
    "        Parameters:\n",
    "        - img_mask_tuples (list or tuple): A list of tuples, where each tuple contains the file paths to an image and its corresponding mask.\n",
    "        \"\"\"\n",
    "        self.img_mask_tuples = img_mask_tuples\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of image-mask tuples in the dataset.\n",
    "\n",
    "        Returns:\n",
    "        - int: The number of image-mask tuples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.img_mask_tuples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Fetches and preprocesses the image-mask pair at the given index.\n",
    "\n",
    "        Parameters:\n",
    "        - idx (int): Index of the image-mask pair to retrieve.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor, torch.Tensor: A tuple containing the preprocessed image and mask as Torch Tensors.\n",
    "        \"\"\"\n",
    "\n",
    "        # Step 1: Fetch the file paths of the image and mask at the given index\n",
    "        img_path, mask_path = self.img_mask_tuples[idx]\n",
    "        \n",
    "        # Step 2: Load and preprocess the image using the 'get_tiff_image' function\n",
    "        image = get_tiff_image(img_path)\n",
    "        \n",
    "        # Step 3: Load and preprocess the mask using the 'get_tiff_image' function with normalization disabled\n",
    "        mask = get_tiff_image(mask_path, normalized=False)\n",
    "        \n",
    "        # Step 4: Convert the mask to a binary mask by thresholding (values > 0 become 1, otherwise 0)\n",
    "        mask[mask > 0] = 1\n",
    "        \n",
    "        return image, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BCDataset(train_tuples)\n",
    "test_dataset = BCDataset(test_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "# Classe DoubleConv\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "\n",
    "        # Si mid_channels n'est pas spécifié, le nombre de canaux intermédiaires est égal au nombre de canaux de sortie.\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "\n",
    "        # Définition de la séquence de couches pour DoubleConv\n",
    "        # La séquence comprend deux convolutions suivies de batch normalization (BN) et de ReLU.\n",
    "        # La première convolution réduit le nombre de canaux à 'mid_channels'.\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # La deuxième convolution ramène le nombre de canaux à 'out_channels' (nombre final de canaux de sortie).\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Passage avant de la couche DoubleConv\n",
    "        # L'entrée 'x' passe à travers les deux convolutions suivies de BN et ReLU, puis la sortie est renvoyée.\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "# Classe Down\n",
    "# Le downscaling ou réduction d'échelle est un processus de \n",
    "# diminution de la résolution spatiale d'une image ou d'une représentation numérique.\n",
    "# Cela signifie qu'une image de départ, qui peut être de grande taille, est réduite \n",
    "# en taille en diminuant le nombre de pixels qu'elle contient.\n",
    "# Le résultat est une image plus petite avec une résolution spatiale inférieure, ce \n",
    "# qui permet de réduire la complexité du traitement et de conserver uniquement les informations les plus importantes de l'image.\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        # Utilisation de MaxPool suivi de DoubleConv pour le downscaling\n",
    "        # MaxPool2d effectue un échantillonnage vers le bas en utilisant une fenêtre de 2x2 avec un décalage de 2 pixels.\n",
    "        # Il réduit la taille spatiale de l'entrée de moitié.\n",
    "        # Ensuite, l'objet DoubleConv est utilisé pour appliquer deux convolutions pour extraire des fonctionnalités.\n",
    "        # DoubleConv est défini dans une autre classe et comprend deux convolutions, suivies de batch normalization et ReLU.\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Passage avant de la couche Down\n",
    "        # L'entrée 'x' est soumise à la séquence de MaxPool et DoubleConv, puis la sortie est renvoyée.\n",
    "        return self.maxpool_conv(x)\n",
    "        \n",
    "# Classe Up\n",
    "# L'upscaling ou augmentation d'échelle est le processus inverse du downscaling.\n",
    "# Il consiste à augmenter la résolution spatiale d'une image ou d'une représentation numérique.\n",
    "# Cela est généralement fait en utilisant des techniques d'interpolation pour estimer les valeurs\n",
    "# des nouveaux pixels ajoutés à l'image agrandie. L'upscaling est souvent utilisé pour améliorer\n",
    "# la résolution d'une image, mais il est important de noter que cela ne restaure pas les détails perdus lors du downscaling.\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # Si bilinear=True, utilise les convolutions normales pour réduire le nombre de canaux\n",
    "        if bilinear:\n",
    "            # Upsample avec le mode bilinéaire pour augmenter la taille spatiale de l'entrée de 2 fois\n",
    "            # et align_corners=True pour assurer l'alignement des coins pendant l'interpolation.\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "            # DoubleConv avec 'in_channels' en entrée, 'out_channels' en sortie et 'in_channels // 2' pour le nombre de canaux intermédiaires.\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            # Si bilinear=False, utilise la transposition de convolution pour augmenter la taille spatiale de l'entrée de 2 fois.\n",
    "            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n",
    "\n",
    "            # DoubleConv avec 'in_channels' en entrée et 'out_channels' en sortie.\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # Upsample de x1 à la taille de x2 et concaténation des deux\n",
    "        x1 = self.up(x1)\n",
    "\n",
    "        # L'entrée est de la forme CHW, calcul des différences de taille spatiale entre x2 et x1.\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        # Application de padding pour que les deux tensors aient les mêmes dimensions.\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "\n",
    "        # Concaténation le long de la dimension des canaux (dim=1) pour fusionner les informations des deux entrées.\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "\n",
    "        # Passage avant de la couche Up, c'est-à-dire la couche de convolution double pour combiner les caractéristiques.\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "# Classe UNet\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        # Définition des couches du modèle UNet\n",
    "        # L'architecture UNet comprend une partie de descente (downsampling) et une partie d'ascension (upsampling).\n",
    "\n",
    "        # Couche d'entrée\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "\n",
    "        # Couches de descente (downsampling)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(512, 1024 // factor)\n",
    "\n",
    "        # Couches d'ascension (upsampling)\n",
    "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
    "        self.up2 = Up(512, 256 // factor, bilinear)\n",
    "        self.up3 = Up(256, 128 // factor, bilinear)\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "\n",
    "        # Couche de sortie\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Passage avant du modèle UNet en utilisant les couches définies\n",
    "\n",
    "        # Couche d'entrée\n",
    "        x1 = self.inc(x)\n",
    "\n",
    "        # Couches de descente (downsampling)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "\n",
    "        # Couches d'ascension (upsampling)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "\n",
    "        # Couche de sortie\n",
    "        logits = self.outc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = UNet(n_channels=3, n_classes=1)\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.02, momentum=0.5)\n",
    "\n",
    "def dice_loss(inputs, target):\n",
    "    inputs = torch.sigmoid(inputs)\n",
    "    smooth = 1.\n",
    "    iflat = inputs.contiguous().view(-1)\n",
    "    tflat = target.contiguous().view(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "    loss = 1 - ((2. * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|███▎                                                                                                                                                                  | 2/100 [2:27:56<120:49:20, 4438.37s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, labels)\n\u001b[1;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 19\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Bureau/simulateurs/virtualEnv/simuEnv/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Bureau/simulateurs/virtualEnv/simuEnv/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(100)):\n",
    "    for images, labels in iter(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        images = images.permute(0,3,2,1).float()\n",
    "        labels = labels.permute(0,3,2,1).float()\n",
    "        labels = labels.sum(1,keepdim=True).bool().float()\n",
    "\n",
    "        output = model(images)\n",
    "        \n",
    "        # process output by a threshold\n",
    "        #output = torch.sigmoid(output)\n",
    "        #output[output >= 0.5] = 1\n",
    "        #output[output < 0.5] = 0\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process output by a threshold\n",
    "output = torch.sigmoid(output)\n",
    "output[output >= 0.5] = 1\n",
    "output[output < 0.5] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_index=0\n",
    "print(\"image\")\n",
    "plt.imshow(images.cpu().detach().numpy()[batch_index,0])\n",
    "plt.show()\n",
    "print(\"output\")\n",
    "plt.imshow(output.cpu().detach().numpy()[batch_index,0])\n",
    "plt.show()\n",
    "print(\"label\")\n",
    "plt.imshow(labels.cpu().detach().numpy()[batch_index,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
